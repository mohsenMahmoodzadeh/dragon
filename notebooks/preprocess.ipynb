{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smPmslU8bQem",
        "outputId": "0a9cab3f-247e-4dcd-f5ea-e70eb974ef4b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bOl8_z5kZDFu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sampling_spec():\n",
        "    label_samples = {\n",
        "        \"Propaganda\": {\n",
        "            'Not Propaganda': 200,\n",
        "            'Propaganda': 107,\n",
        "            'Unclear': 40,\n",
        "            'Not Applicable': 17,\n",
        "        },\n",
        "        \"Bias\": {\n",
        "            'Biased against both Palestine and Israel': 2,\n",
        "            'Unclear': 5,\n",
        "            'Biased against others': 5,\n",
        "            'Not Applicable': 8,\n",
        "            'Biased against Israel': 13,\n",
        "            'Biased against Palestine': 22,\n",
        "            'Unbiased': 159\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return label_samples\n"
      ],
      "metadata": {
        "id": "Tn6g41uOZLFm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_from_label(data: pd.DataFrame, task: str, label_name: str, num_sample: int):\n",
        "    label_data = data[data[f'{task}'] == label_name]\n",
        "    sample_data = label_data.sample(n=num_sample, random_state=1)\n",
        "    return sample_data"
      ],
      "metadata": {
        "id": "1rQ5uxHOZMFq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    data = data.drop(columns=['Unnamed: 0', 'Comments', 'Annotator ID'])\n",
        "    assert data.isna().any().sum() == 0\n",
        "    data = data.drop_duplicates()\n",
        "    data_copy = data.copy()\n",
        "\n",
        "    data_copy[\"text_tokens_count\"] = data_copy[\"Text\"].str.replace(',', '').str.split().str.len()\n",
        "    data_copy[\"text_length\"] = data_copy[\"Text\"].str.len()\n",
        "\n",
        "    return data_copy"
      ],
      "metadata": {
        "id": "wGUwem7zZMvW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_train_test_data(data: pd.DataFrame, label_samples: dict, task: str) -> dict:\n",
        "    test_data = pd.DataFrame()\n",
        "    train_data = pd.DataFrame()\n",
        "\n",
        "    for label, sample_size in label_samples.items():\n",
        "        sample_data = sample_from_label(data, task, label, sample_size)\n",
        "        remain_data = pd.concat(\n",
        "            [data[data[f'{task}'] == label], sample_data]\n",
        "        ).drop_duplicates(keep=False)\n",
        "\n",
        "        train_data = pd.concat([train_data, remain_data])\n",
        "        test_data = pd.concat([test_data, sample_data])\n",
        "\n",
        "    train_data = train_data.reset_index(drop=True)\n",
        "    test_data = test_data.reset_index(drop=True)\n",
        "    assert train_data.shape[0] + test_data.shape[0] == data.shape[0]\n",
        "    data_splits = {\"train\": train_data, \"test\": test_data}\n",
        "\n",
        "    return data_splits"
      ],
      "metadata": {
        "id": "0mS3ZpUwZOkY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    # Use one of the following values for task parameter: 'Propaganda' or 'Bias'\n",
        "    task = 'Propaganda'\n",
        "    label_samples = get_sampling_spec()\n",
        "\n",
        "    data_dir = os.path.join(os.getcwd(), 'data')\n",
        "    Path(data_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    subset_data = pd.read_excel(os.path.join(data_dir, 'annotated_data.xlsx'))\n",
        "    preprocessed_data = preprocess(subset_data)\n",
        "    task_data_splits = prepare_train_test_data(preprocessed_data, label_samples[task], task)\n",
        "\n",
        "    task_data_dir = os.path.join(data_dir, task.lower())\n",
        "    Path(task_data_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    task_data_splits[\"train\"].to_excel(\n",
        "        os.path.join(task_data_dir, f\"{task.lower()}_train_data.xlsx\"),\n",
        "        index=False\n",
        "    )\n",
        "    task_data_splits[\"test\"].to_excel(\n",
        "        os.path.join(task_data_dir, f\"{task.lower()}_test_data.xlsx\"),\n",
        "        index=False\n",
        "    )"
      ],
      "metadata": {
        "id": "eoe65GWqZR_x"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "lJlfEl-sZU4R"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V0HWDSwHZlvt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}